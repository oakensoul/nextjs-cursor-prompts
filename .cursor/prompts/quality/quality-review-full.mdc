---
description:
globs:
alwaysApply: false
---
# Comprehensive Quality Review Orchestrator

**This prompt orchestrates a complete quality assessment across all project dimensions including code quality, security, accessibility, API design, SEO optimization, and documentation with consolidated reporting, prioritized action plans, and persistent quality documentation tracking.**

## üéØ **COMPREHENSIVE REVIEW OBJECTIVES:**

1. **ORCHESTRATE multi-dimensional quality assessment** across all project areas
2. **COORDINATE specialized reviews** with consistent methodology and standards
3. **CONSOLIDATE findings** into actionable, prioritized improvement roadmap
4. **ESTABLISH quality baseline** with measurable metrics and tracking
5. **GENERATE executive summary** with business impact and resource planning
6. **CREATE persistent documentation** in docs/quality-review/ for historical tracking

## üìã **QUALITY ASSESSMENT ORCHESTRATION:**

### **STEP 1: Pre-Assessment Setup and Planning**

#### **Assessment Workspace and Documentation Setup:**
```bash
# Initialize comprehensive quality assessment with documentation tracking
echo "üîç Initializing Comprehensive Quality Assessment with Documentation Tracking"
echo "=========================================================================="

# Create assessment workspace AND persistent documentation
ASSESSMENT_DATE=$(date +"%Y%m%d_%H%M%S")
PROJECT_NAME=$(basename $(pwd))
ASSESSMENT_DIR="quality_assessment_${ASSESSMENT_DATE}"
DOCS_QUALITY_DIR="docs/quality-review"

# Create both temporary workspace and persistent documentation
mkdir -p "$ASSESSMENT_DIR"/{reports,artifacts,recommendations}
mkdir -p "$DOCS_QUALITY_DIR"/{reports,historical,summaries}

echo "üìÅ Temporary workspace: $ASSESSMENT_DIR"
echo "üìö Persistent documentation: $DOCS_QUALITY_DIR"
echo "üìÖ Assessment date: $(date)"
echo "üë§ Assessed by: $(git config user.name) <$(git config user.email)>"
echo "üìÇ Project: $PROJECT_NAME"

# Project inventory
echo -e "\nüèóÔ∏è Project Overview:"
echo "Repository: $(git remote get-url origin 2>/dev/null || echo 'Local repository')"
echo "Current branch: $(git branch --show-current)"
echo "Last commit: $(git log -1 --oneline)"
echo "Files tracked: $(git ls-files | wc -l)"

# Technology stack detection
echo -e "\nüõ†Ô∏è Technology Stack:"
[ -f "package.json" ] && echo "- Node.js/JavaScript project detected"
[ -f "tsconfig.json" ] && echo "- TypeScript configuration found"
[ -f "next.config.js" ] && echo "- Next.js framework detected"
[ -f "tailwind.config.js" ] && echo "- Tailwind CSS detected"
[ -f "vitest.config.ts" ] && echo "- Vitest testing framework detected"
[ -f "cypress.config.js" ] && echo "- Cypress E2E testing detected"
[ -f ".eslintrc*" ] && echo "- ESLint configuration found"
[ -f "prettier.config.js" ] && echo "- Prettier configuration found"
```

#### **Assessment Configuration:**
```markdown
## Quality Assessment Configuration

### Assessment Scope:
- [ ] **Code Quality Review** - Code standards, maintainability, technical debt
- [ ] **Security Assessment** - Vulnerability analysis, security practices
- [ ] **Accessibility Audit** - WCAG compliance, inclusive design
- [ ] **SEO Optimization** - Technical SEO, meta tags, structured data, performance
- [ ] **API Quality Review** - API design, documentation, consistency
- [ ] **Documentation Review** - Completeness, accuracy, user experience

### Assessment Depth:
- **Sampling Strategy**: [Full/Representative Sample/Critical Paths Only]
- **Review Intensity**: [Deep Dive/Standard/High-Level]
- **Focus Areas**: [List specific areas of concern]
- **Exclusions**: [Areas explicitly excluded from review]

### Success Criteria:
- **Quality Baseline**: Establish current quality metrics
- **Improvement Roadmap**: Prioritized action plan
- **Resource Estimation**: Time and effort requirements
- **Risk Assessment**: Quality-related business risks
- **Documentation Tracking**: Persistent quality history in docs/quality-review/
```

### **STEP 2: Coordinated Quality Review Execution**

#### **Sequential Quality Assessment Execution:**
```bash
# Execute comprehensive quality reviews including SEO
echo "üöÄ Executing Coordinated Quality Reviews (Including SEO)"
echo "======================================================"

CURRENT_DIR=$(pwd)
ASSESSMENT_LOG="$ASSESSMENT_DIR/assessment_execution.log"
DOCS_LOG="$DOCS_QUALITY_DIR/assessment_history.log"

# Function to run individual quality review with documentation
run_quality_review() {
    local review_type="$1"
    local review_prompt="$2"
    local temp_output="$3"
    local docs_output="$4"
    
    echo "‚è±Ô∏è Starting $review_type review..." | tee -a "$ASSESSMENT_LOG" | tee -a "$DOCS_LOG"
    echo "üìù Prompt: $review_prompt" | tee -a "$ASSESSMENT_LOG"
    echo "üìÑ Temp Output: $temp_output" | tee -a "$ASSESSMENT_LOG"
    echo "üìö Docs Output: $docs_output" | tee -a "$ASSESSMENT_LOG"
    
    # Note: In actual implementation, this would trigger the specific review prompt
    echo "üîÑ Executing .cursor/prompts/quality/$review_prompt" | tee -a "$ASSESSMENT_LOG"
    echo "‚úÖ $review_type review completed" | tee -a "$ASSESSMENT_LOG"
    echo "üìã Results saved to both temporary workspace and persistent docs" | tee -a "$ASSESSMENT_LOG"
    echo "---" | tee -a "$ASSESSMENT_LOG"
}

# Execute individual quality reviews in sequence
echo "üìä Review Execution Plan:"
echo "1. Code Quality Review"
echo "2. Security Assessment" 
echo "3. Accessibility Audit"
echo "4. SEO Optimization Audit"
echo "5. API Quality Review"
echo "6. Documentation Review"
echo ""

# 1. Code Quality Review
run_quality_review "Code Quality" "quality-review-code.mdc" \
    "$ASSESSMENT_DIR/reports/code_quality_report.md" \
    "$DOCS_QUALITY_DIR/reports/${ASSESSMENT_DATE}_code_quality.md"

# 2. Security Assessment
run_quality_review "Security" "quality-review-security.mdc" \
    "$ASSESSMENT_DIR/reports/security_assessment_report.md" \
    "$DOCS_QUALITY_DIR/reports/${ASSESSMENT_DATE}_security.md"

# 3. Accessibility Audit
run_quality_review "Accessibility" "quality-review-accessibility.mdc" \
    "$ASSESSMENT_DIR/reports/accessibility_audit_report.md" \
    "$DOCS_QUALITY_DIR/reports/${ASSESSMENT_DATE}_accessibility.md"

# 4. SEO Optimization Audit
run_quality_review "SEO Optimization" "quality-review-seo.mdc" \
    "$ASSESSMENT_DIR/reports/seo_audit_report.md" \
    "$DOCS_QUALITY_DIR/reports/${ASSESSMENT_DATE}_seo.md"

# 5. API Quality Review
run_quality_review "API Quality" "quality-review-api.mdc" \
    "$ASSESSMENT_DIR/reports/api_quality_report.md" \
    "$DOCS_QUALITY_DIR/reports/${ASSESSMENT_DATE}_api.md"

# 6. Documentation Review
run_quality_review "Documentation" "quality-review-documentation.mdc" \
    "$ASSESSMENT_DIR/reports/documentation_review_report.md" \
    "$DOCS_QUALITY_DIR/reports/${ASSESSMENT_DATE}_documentation.md"

echo "‚úÖ All individual quality reviews completed"
echo "üìÅ Temporary reports saved to: $ASSESSMENT_DIR/reports/"
echo "üìö Persistent documentation saved to: $DOCS_QUALITY_DIR/reports/"
```

#### **Cross-Review Correlation Analysis:**
```bash
# Analyze cross-cutting quality themes including SEO
echo "üîó Analyzing Cross-Review Correlations (Including SEO)"
echo "===================================================="

# Identify common themes across reviews including SEO
cat > "$ASSESSMENT_DIR/artifacts/correlation_analysis.md" << 'EOF'
# Cross-Review Correlation Analysis

## Common Quality Themes Identified

### Technical Debt Indicators:
- [ ] Outdated dependencies (Security + Code Quality)
- [ ] Legacy code patterns (Code Quality + Accessibility)
- [ ] Inconsistent error handling (Code Quality + API Quality)
- [ ] Missing documentation (Documentation + API Quality)
- [ ] Poor semantic HTML (Accessibility + SEO)

### Security and Accessibility Overlap:
- [ ] Input validation issues (Security + Accessibility)
- [ ] Authentication patterns (Security + API Quality)
- [ ] Error message exposure (Security + Code Quality)
- [ ] Data handling practices (Security + Code Quality)

### SEO and Performance Correlations:
- [ ] Page load performance (SEO + Code Quality)
- [ ] Mobile responsiveness (SEO + Accessibility)
- [ ] Semantic HTML structure (SEO + Accessibility)
- [ ] Meta tag consistency (SEO + Documentation)
- [ ] Image optimization (SEO + Code Quality)
- [ ] Core Web Vitals (SEO + Code Quality)

### User Experience Consistency:
- [ ] API design patterns (API Quality + Documentation)
- [ ] Error handling user experience (API Quality + Accessibility)
- [ ] Documentation accessibility (Documentation + Accessibility)
- [ ] Performance implications (Code Quality + Accessibility + SEO)
- [ ] Content discoverability (SEO + Documentation)

### Maintenance and Sustainability:
- [ ] Code maintainability (Code Quality + Documentation)
- [ ] Security update processes (Security + Code Quality)
- [ ] Documentation currency (Documentation + All Reviews)
- [ ] Testing coverage gaps (Code Quality + Security + Accessibility)
- [ ] SEO monitoring and tracking (SEO + Documentation)
EOF

# Copy correlation analysis to persistent docs
cp "$ASSESSMENT_DIR/artifacts/correlation_analysis.md" "$DOCS_QUALITY_DIR/summaries/${ASSESSMENT_DATE}_correlations.md"
```

### **STEP 3: Quality Metrics Consolidation**

#### **Quantitative Quality Metrics (Including SEO):**
```bash
# Consolidate quantitative metrics from all reviews including SEO
echo "üìà Consolidating Quality Metrics (Including SEO)"
echo "=============================================="

cat > "$ASSESSMENT_DIR/artifacts/quality_metrics.json" << EOF
{
  "assessment_metadata": {
    "date": "$(date -Iseconds)",
    "project": "$(basename $(pwd))",
    "assessor": "$(git config user.name)",
    "commit": "$(git rev-parse HEAD)",
    "assessment_id": "${ASSESSMENT_DATE}"
  },
  "code_quality": {
    "technical_debt_ratio": null,
    "code_coverage": null,
    "maintainability_index": null,
    "cyclomatic_complexity": null,
    "eslint_issues": null,
    "typescript_errors": null
  },
  "security": {
    "vulnerability_count": null,
    "security_score": null,
    "dependency_audit_issues": null,
    "authentication_coverage": null,
    "input_validation_coverage": null
  },
  "accessibility": {
    "wcag_compliance_level": null,
    "accessibility_violations": null,
    "keyboard_navigation_coverage": null,
    "screen_reader_compatibility": null,
    "color_contrast_issues": null
  },
  "seo": {
    "lighthouse_seo_score": null,
    "core_web_vitals_lcp": null,
    "core_web_vitals_fid": null,
    "core_web_vitals_cls": null,
    "meta_tags_completeness": null,
    "structured_data_coverage": null,
    "mobile_usability_score": null,
    "page_speed_score": null,
    "internal_linking_score": null,
    "content_optimization_score": null
  },
  "api_quality": {
    "endpoint_documentation_coverage": null,
    "api_consistency_score": null,
    "error_handling_completeness": null,
    "response_time_compliance": null,
    "versioning_strategy_completeness": null
  },
  "documentation": {
    "documentation_coverage": null,
    "documentation_freshness": null,
    "user_path_completeness": null,
    "example_accuracy": null,
    "maintenance_sustainability": null
  }
}
EOF

# Copy metrics to persistent docs
cp "$ASSESSMENT_DIR/artifacts/quality_metrics.json" "$DOCS_QUALITY_DIR/summaries/${ASSESSMENT_DATE}_metrics.json"

echo "üìä Quality metrics template created: $ASSESSMENT_DIR/artifacts/quality_metrics.json"
echo "üìö Metrics copied to persistent docs: $DOCS_QUALITY_DIR/summaries/${ASSESSMENT_DATE}_metrics.json"
echo "üìù Note: Metrics to be populated from individual review reports"
```

#### **Quality Score Calculation:**
```bash
# Calculate composite quality scores
cat > "$ASSESSMENT_DIR/artifacts/quality_scoring.md" << 'EOF'
# Quality Scoring Methodology

## Scoring Framework

### Individual Review Scores (0-100):
- **Code Quality Score**: [TBD based on review findings]
- **Security Score**: [TBD based on vulnerability assessment]
- **Accessibility Score**: [TBD based on WCAG compliance]
- **SEO Score**: [TBD based on SEO audit]
- **API Quality Score**: [TBD based on design and documentation]
- **Documentation Score**: [TBD based on completeness and usability]

### Weighted Composite Score:
```
Composite Quality Score = (
  Code Quality √ó 0.20 +
  Security √ó 0.20 +
  Accessibility √ó 0.15 +
  SEO √ó 0.15 +
  API Quality √ó 0.15 +
  Documentation √ó 0.15
)
```

### Quality Grade Mapping:
- **90-100**: Excellent (A)
- **80-89**: Good (B)
- **70-79**: Fair (C)
- **60-69**: Poor (D)
- **0-59**: Critical (F)

### Risk Level Assessment:
- **Low Risk**: Score 80+, no critical issues
- **Medium Risk**: Score 60-79, some critical issues
- **High Risk**: Score 40-59, multiple critical issues
- **Critical Risk**: Score <40, widespread critical issues
EOF
```

### **STEP 4: Consolidated Findings Analysis**

#### **Priority Matrix Generation (Including SEO):**
```bash
# Generate priority matrix for all findings including SEO
echo "üéØ Generating Priority Matrix (Including SEO)"
echo "==========================================="

cat > "$ASSESSMENT_DIR/artifacts/priority_matrix.md" << 'EOF'
# Quality Issues Priority Matrix

## Priority Classification Framework

### Critical (P0) - Immediate Action Required:
**Business Impact**: High | **Technical Effort**: Any
- [ ] Security vulnerabilities (CVSS 7.0+)
- [ ] Accessibility violations (WCAG A/AA blockers)
- [ ] SEO critical issues (missing title tags, broken structured data)
- [ ] API breaking changes or failures
- [ ] Documentation gaps affecting user onboarding
- [ ] Code quality issues causing production risks

### High (P1) - Address Within Sprint:
**Business Impact**: Medium-High | **Technical Effort**: Low-Medium
- [ ] Performance degradation issues
- [ ] Moderate security concerns (CVSS 4.0-6.9)
- [ ] Accessibility improvements (WCAG AAA)
- [ ] SEO optimization opportunities (Core Web Vitals, meta descriptions)
- [ ] API consistency improvements
- [ ] Documentation accuracy issues

### Medium (P2) - Address Within Month:
**Business Impact**: Medium | **Technical Effort**: Medium
- [ ] Code maintainability improvements
- [ ] Security hardening measures
- [ ] Enhanced accessibility features
- [ ] SEO content optimization and structured data enhancements
- [ ] API optimization opportunities
- [ ] Documentation completeness gaps

### Low (P3) - Address When Capacity Allows:
**Business Impact**: Low | **Technical Effort**: Low-High
- [ ] Code style and formatting issues
- [ ] Preventive security measures
- [ ] Accessibility nice-to-haves
- [ ] SEO fine-tuning and advanced optimizations
- [ ] API documentation enhancements
- [ ] Documentation polish and examples

## Resource Estimation by Priority:
- **P0 Issues**: Immediate sprint allocation
- **P1 Issues**: Next sprint planning
- **P2 Issues**: Quarterly planning cycle
- **P3 Issues**: Backlog for capacity planning

## SEO-Specific Priority Considerations:
- **Core Web Vitals issues**: High priority (affects search rankings)
- **Missing meta tags**: Critical priority (basic SEO requirement)
- **Structured data errors**: Medium priority (rich results opportunity)
- **Mobile usability**: High priority (mobile-first indexing)
- **Content optimization**: Medium priority (long-term organic growth)
EOF

# Copy priority matrix to persistent docs
cp "$ASSESSMENT_DIR/artifacts/priority_matrix.md" "$DOCS_QUALITY_DIR/summaries/${ASSESSMENT_DATE}_priorities.md"
```

#### **Risk Assessment Consolidation:**
```bash
# Consolidate risk assessment across all quality dimensions
cat > "$ASSESSMENT_DIR/artifacts/risk_assessment.md" << 'EOF'
# Consolidated Risk Assessment

## Business Risk Analysis

### Security Risks:
- **Data Breach Risk**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Compliance Risk**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Availability Risk**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Reputation Risk**: [LOW/MEDIUM/HIGH/CRITICAL]

### Operational Risks:
- **Maintenance Burden**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Developer Productivity**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Technical Debt Accumulation**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Knowledge Transfer Risk**: [LOW/MEDIUM/HIGH/CRITICAL]

### User Experience Risks:
- **Accessibility Exclusion**: [LOW/MEDIUM/HIGH/CRITICAL]
- **API Integration Difficulty**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Documentation Usability**: [LOW/MEDIUM/HIGH/CRITICAL]
- **Performance Impact**: [LOW/MEDIUM/HIGH/CRITICAL]

## Risk Mitigation Priorities:
1. [Highest risk area requiring immediate attention]
2. [Second priority risk area]
3. [Third priority risk area]

## Risk Monitoring Plan:
- **Review Frequency**: [Weekly/Monthly/Quarterly]
- **Key Metrics**: [List metrics to track]
- **Escalation Triggers**: [Conditions requiring escalation]
EOF
```

### **STEP 5: Executive Summary and Recommendations**

#### **Executive Summary Generation (Including SEO):**
```bash
# Generate executive summary for stakeholders including SEO
echo "üìä Generating Executive Summary (Including SEO)"
echo "============================================="

cat > "$ASSESSMENT_DIR/EXECUTIVE_SUMMARY.md" << EOF
# Comprehensive Quality Assessment - Executive Summary

**Assessment Date**: $(date)
**Project**: $(basename $(pwd))
**Assessment Scope**: Full-spectrum quality review including SEO
**Assessed By**: $(git config user.name) <$(git config user.email)>
**Assessment ID**: ${ASSESSMENT_DATE}

## üéØ **Quality Overview**

### Overall Quality Health: üü° NEEDS_IMPROVEMENT
### Composite Quality Score: [TBD]/100 (Grade: [TBD])
### Risk Level: [LOW/MEDIUM/HIGH/CRITICAL]

## üìä **Quality Dimensions Summary**

| Dimension | Score | Grade | Status | Priority Issues |
|-----------|-------|-------|---------|-----------------|
| Code Quality | [TBD]/100 | [TBD] | üü° | [Count] issues |
| Security | [TBD]/100 | [TBD] | üü° | [Count] issues |
| Accessibility | [TBD]/100 | [TBD] | üü° | [Count] issues |
| SEO Optimization | [TBD]/100 | [TBD] | üü° | [Count] issues |
| API Quality | [TBD]/100 | [TBD] | üü° | [Count] issues |
| Documentation | [TBD]/100 | [TBD] | üü° | [Count] issues |

## üö® **Critical Findings** (P0 - Immediate Action)
1. [Critical issue 1 - Business impact and recommended action]
2. [Critical issue 2 - Business impact and recommended action]
3. [Critical issue 3 - Business impact and recommended action]

## üîç **SEO Health Summary**
### Technical SEO Status:
- **Core Web Vitals**: [GOOD/NEEDS IMPROVEMENT/POOR]
- **Mobile Usability**: [PASS/FAIL]
- **Structured Data**: [VALID/ERRORS FOUND]
- **Meta Tags Coverage**: [%] complete
- **Lighthouse SEO Score**: [TBD]/100

### SEO Priority Actions:
1. [High-impact SEO improvement 1]
2. [High-impact SEO improvement 2]
3. [High-impact SEO improvement 3]

## üìà **Key Recommendations**

### Immediate Actions (Next 2 weeks):
- [ ] [High-impact, immediate action 1]
- [ ] [High-impact, immediate action 2]
- [ ] [SEO critical fix if applicable]

### Short-term Improvements (Next 1 month):
- [ ] [Medium-impact improvement 1]
- [ ] [Medium-impact improvement 2]
- [ ] [SEO optimization opportunity]

### Strategic Investments (Next 3 months):
- [ ] [Long-term quality infrastructure]
- [ ] [Preventive quality measures]
- [ ] [SEO monitoring and automation setup]

## üí∞ **Resource Requirements**

### Immediate Actions:
- **Development Time**: [X] developer days
- **Specialist Time**: [X] security/accessibility/SEO specialist days
- **Total Estimated Effort**: [X] developer days

### Skill Requirements:
- [ ] Senior Developer (Code Quality, Architecture)
- [ ] Security Specialist (Vulnerability remediation)
- [ ] UX/Accessibility Expert (Accessibility improvements)
- [ ] SEO Specialist (Technical SEO optimization)
- [ ] Technical Writer (Documentation improvements)
- [ ] DevOps Engineer (Infrastructure/API improvements)

## üìã **Documentation and Tracking**

### Quality Review Documentation:
- **Persistent Reports**: docs/quality-review/reports/
- **Historical Metrics**: docs/quality-review/summaries/
- **Assessment History**: docs/quality-review/assessment_history.log
- **Next Review**: [Recommended date]

### Continuous Monitoring Setup:
- [ ] Automated quality checks in CI/CD
- [ ] SEO monitoring dashboard
- [ ] Regular quality review schedule
- [ ] Quality metrics tracking

---
**Assessment Type**: Comprehensive Quality Review
**Generated By**: quality-review-full.mdc v3.0.0
**Review ID**: quality_assessment_${ASSESSMENT_DATE}
**Persistent Documentation**: docs/quality-review/
EOF

# Copy executive summary to persistent docs
cp "$ASSESSMENT_DIR/EXECUTIVE_SUMMARY.md" "$DOCS_QUALITY_DIR/summaries/${ASSESSMENT_DATE}_executive_summary.md"

echo "üìÑ Executive summary generated: $ASSESSMENT_DIR/EXECUTIVE_SUMMARY.md"
echo "üìö Executive summary copied to: $DOCS_QUALITY_DIR/summaries/${ASSESSMENT_DATE}_executive_summary.md"
```

#### **Quality Review Documentation Index Creation:**
```bash
# Create or update quality review documentation index
echo "üìö Creating Quality Review Documentation Index"
echo "============================================"

cat > "$DOCS_QUALITY_DIR/README.md" << 'EOF'
# Quality Review Documentation

This directory contains comprehensive quality review documentation for tracking project quality over time.

## üìÅ **Directory Structure**

```
docs/quality-review/
‚îú‚îÄ‚îÄ README.md                    # This index file
‚îú‚îÄ‚îÄ assessment_history.log       # Complete assessment history
‚îú‚îÄ‚îÄ reports/                     # Individual review reports
‚îÇ   ‚îú‚îÄ‚îÄ [date]_code_quality.md
‚îÇ   ‚îú‚îÄ‚îÄ [date]_security.md
‚îÇ   ‚îú‚îÄ‚îÄ [date]_accessibility.md
‚îÇ   ‚îú‚îÄ‚îÄ [date]_seo.md
‚îÇ   ‚îú‚îÄ‚îÄ [date]_api.md
‚îÇ   ‚îî‚îÄ‚îÄ [date]_documentation.md
‚îú‚îÄ‚îÄ summaries/                   # Executive summaries and metrics
‚îÇ   ‚îú‚îÄ‚îÄ [date]_executive_summary.md
‚îÇ   ‚îú‚îÄ‚îÄ [date]_metrics.json
‚îÇ   ‚îú‚îÄ‚îÄ [date]_priorities.md
‚îÇ   ‚îî‚îÄ‚îÄ [date]_correlations.md
‚îî‚îÄ‚îÄ historical/                  # Archived assessments
    ‚îî‚îÄ‚îÄ [year]/
        ‚îî‚îÄ‚îÄ [archived files]
```

## üìä **Latest Assessment**

**Last Assessment**: [ASSESSMENT_DATE]
**Overall Quality Score**: [TBD]/100
**Risk Level**: [TBD]
**Next Recommended Review**: [TBD]

## üìà **Quality Tracking**

### Assessment History:
- [Previous assessments will be listed here]

### Quality Trends:
- [Quality improvement/degradation trends]

### Key Metrics Tracking:
- Code Quality Score: [Historical trend]
- Security Score: [Historical trend] 
- Accessibility Score: [Historical trend]
- SEO Score: [Historical trend]
- API Quality Score: [Historical trend]
- Documentation Score: [Historical trend]

## üéØ **Quality Goals**

### Current Quarter Objectives:
- [ ] [Quality goal 1]
- [ ] [Quality goal 2]
- [ ] [Quality goal 3]

### Annual Quality Targets:
- **Code Quality**: Target [X]/100
- **Security**: Target [X]/100
- **Accessibility**: Target [X]/100
- **SEO**: Target [X]/100
- **API Quality**: Target [X]/100
- **Documentation**: Target [X]/100

## üîÑ **Review Schedule**

- **Comprehensive Reviews**: Quarterly
- **Focused Reviews**: Monthly
- **Continuous Monitoring**: Weekly automated checks
- **Emergency Reviews**: As needed for critical issues

## üìã **Using This Documentation**

### For Developers:
- Review latest assessment findings before feature work
- Reference specific quality reports for improvement guidance
- Track progress on assigned quality improvement tasks

### For Team Leads:
- Use executive summaries for planning and resource allocation
- Monitor quality trends for team performance insights
- Reference priority matrices for sprint planning

### For Stakeholders:
- Review executive summaries for quality health overview
- Track quality metrics against business objectives
- Understand resource requirements for quality investments

---
*Generated by quality-review-full.mdc v3.0.0*
*Last Updated: [ASSESSMENT_DATE]*
EOF

# Update assessment history log
echo "$(date -Iseconds) | Assessment ID: ${ASSESSMENT_DATE} | Type: Comprehensive | Assessor: $(git config user.name)" >> "$DOCS_QUALITY_DIR/assessment_history.log"

echo "üìö Quality review documentation index created: $DOCS_QUALITY_DIR/README.md"
echo "üìù Assessment logged to: $DOCS_QUALITY_DIR/assessment_history.log"
```

## üí° **COMPREHENSIVE REVIEW EXAMPLES:**

### **Example 1: Pre-Launch Quality Assessment with SEO**
```
Perform comprehensive quality review before product launch including SEO readiness

Following .cursor/prompts/quality/quality-review-full.mdc:
- Execute full-spectrum quality assessment including SEO audit
- Focus on security, accessibility, and SEO compliance for launch readiness
- Generate executive summary for stakeholder review
- Create prioritized launch readiness action plan
- Document findings in docs/quality-review/ for future reference
```

### **Example 2: Technical Debt Assessment with Documentation Tracking**
```
Comprehensive quality review to assess technical debt with persistent documentation

Following .cursor/prompts/quality/quality-review-full.mdc:
- Emphasize code quality, SEO performance, and maintainability
- Correlate findings across all quality dimensions including SEO
- Generate resource estimation for debt reduction
- Create quarterly improvement roadmap
- Establish baseline in docs/quality-review/ for trend tracking
```

### **Example 3: Quarterly Quality Health Check with Historical Analysis**
```
Regular comprehensive quality assessment for ongoing monitoring with documentation

Following .cursor/prompts/quality/quality-review-full.mdc:
- Establish quality baseline metrics including SEO performance
- Track improvement from previous assessment in docs/quality-review/
- Update risk assessment and mitigation strategies
- Adjust quality improvement roadmap based on findings
- Maintain quality documentation for team reference and planning
```

---

**This enhanced comprehensive quality review orchestrator ensures systematic evaluation across all quality dimensions including SEO optimization, with consolidated reporting, prioritized recommendations, actionable improvement roadmaps, and persistent documentation tracking in docs/quality-review/ for historical reference and trend analysis.**
